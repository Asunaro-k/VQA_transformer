{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_TOKEN\"] = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "# Constants\n",
    "MAX_VOCAB_SIZE = 10000\n",
    "MAX_SEQUENCE_LENGTH = 20\n",
    "EMBEDDING_DIM = 256\n",
    "LSTM_UNITS = 256\n",
    "NUM_HEADS = 8\n",
    "FF_DIM = 512\n",
    "BATCH_SIZE = 64\n",
    "IMG_SIZE = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, dataframe, tokenizer, img_dir, batch_size=BATCH_SIZE, shuffle=True):\n",
    "        self.df = dataframe\n",
    "        self.batch_size = batch_size\n",
    "        self.tokenizer = tokenizer\n",
    "        self.img_dir = img_dir\n",
    "        self.shuffle = shuffle\n",
    "        self.indexes = np.arange(len(self.df))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.df) / float(self.batch_size)))\n",
    "    \n",
    "    # def __getitem__(self, idx):\n",
    "    #     batch_indexes = self.indexes[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "    #     batch_df = self.df.iloc[batch_indexes]\n",
    "\n",
    "    #     X_image = self.load_images(batch_df['image_id'])\n",
    "    #     X_question = self.tokenizer.texts_to_sequences(batch_df['question_preprocessed'])\n",
    "    #     X_question = pad_sequences(X_question, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    #     y = self.tokenizer.texts_to_sequences(batch_df['answer_preprocessed'])\n",
    "    #     y = pad_sequences(y, maxlen=1)  # 答えは単一のトークン\n",
    "    #     y = y.reshape(-1)  # (batch_size,) の形状に変更\n",
    "\n",
    "    #     return [X_image, X_question], y\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_indexes = self.indexes[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_df = self.df.iloc[batch_indexes]\n",
    "\n",
    "        X_image = self.load_images(batch_df['image_id'])\n",
    "        X_question = self.tokenizer.texts_to_sequences(batch_df['question_preprocessed'])\n",
    "        X_question = pad_sequences(X_question, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "        y = self.tokenizer.texts_to_sequences(batch_df['answer_preprocessed'])\n",
    "        y = pad_sequences(y, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "        \n",
    "        # Convert y to sparse categorical format (use the first token as the answer)\n",
    "        y = y[:, 0]\n",
    "\n",
    "        return [X_image, X_question], y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def load_images(self, image_ids):\n",
    "        images = []\n",
    "        for img_id in image_ids:\n",
    "            path = os.path.join(self.img_dir, f\"{img_id}\")\n",
    "            img = cv2.imread(path)\n",
    "            img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
    "            img = np.array(img) / 255.0\n",
    "            images.append(img)\n",
    "        return np.array(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderLayer(layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(TransformerDecoderLayer, self).__init__()\n",
    "\n",
    "        self.mha1 = layers.MultiHeadAttention(num_heads, d_model)\n",
    "        self.mha2 = layers.MultiHeadAttention(num_heads, d_model)\n",
    "\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            layers.Dense(dff, activation='relu'),\n",
    "            layers.Dense(d_model)\n",
    "        ])\n",
    "\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "        self.dropout3 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, enc_output, training, look_ahead_mask=None, padding_mask=None):\n",
    "        attn1 = self.mha1(query=x, key=x, value=x, attention_mask=look_ahead_mask)\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "\n",
    "        attn2 = self.mha2(query=out1, key=enc_output, value=enc_output, attention_mask=padding_mask)\n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "        out2 = self.layernorm2(attn2 + out1)\n",
    "\n",
    "        ffn_output = self.ffn(out2)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layernorm3(ffn_output + out2)\n",
    "\n",
    "        return out3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1kome\n",
    "class VQAModel(Model):\n",
    "    def __init__(self, vocab_size, max_length):\n",
    "        super(VQAModel, self).__init__()\n",
    "        self.image_model = tf.keras.applications.ResNet50(include_top=False, weights='imagenet')\n",
    "        self.image_model.trainable = False\n",
    "        self.image_dense = layers.Dense(EMBEDDING_DIM, activation='relu')\n",
    "        \n",
    "        self.embedding = layers.Embedding(vocab_size, EMBEDDING_DIM)\n",
    "        self.lstm = layers.LSTM(LSTM_UNITS, return_sequences=True)\n",
    "        \n",
    "        self.decoder_layer = TransformerDecoderLayer(EMBEDDING_DIM, NUM_HEADS, FF_DIM)\n",
    "        self.final_dense = layers.Dense(EMBEDDING_DIM, activation='relu')\n",
    "        self.output_layer = layers.Dense(vocab_size, activation='softmax')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        image, question = inputs\n",
    "        \n",
    "        # Image encoding\n",
    "        image_features = self.image_model(image)\n",
    "        image_features = layers.GlobalAveragePooling2D()(image_features)\n",
    "        image_features = self.image_dense(image_features)\n",
    "        image_features = tf.expand_dims(image_features, 1)\n",
    "        \n",
    "        # Question encoding\n",
    "        embedded_question = self.embedding(question)\n",
    "        question_features = self.lstm(embedded_question)\n",
    "        \n",
    "        # Combine image and question features\n",
    "        decoder_input = tf.concat([image_features, question_features], axis=1)\n",
    "        \n",
    "        # Transformer decoder\n",
    "        decoder_output = self.decoder_layer(decoder_input, decoder_input, training=True)\n",
    "        \n",
    "        # Final processing\n",
    "        output = self.final_dense(decoder_output)\n",
    "        output = tf.reduce_mean(output, axis=1)  # Global average pooling\n",
    "        output = self.output_layer(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2kome\n",
    "class VQAModel(Model):\n",
    "    def __init__(self, vocab_size, max_length):\n",
    "        super(VQAModel, self).__init__()\n",
    "        self.image_model = tf.keras.applications.ResNet50(include_top=False, weights='imagenet')\n",
    "        self.image_model.trainable = False  # 必要に応じてTrue に設定\n",
    "        self.image_dense = layers.Dense(EMBEDDING_DIM, activation='relu')\n",
    "        \n",
    "        self.embedding = layers.Embedding(vocab_size, EMBEDDING_DIM)\n",
    "        self.lstm = layers.Bidirectional(layers.LSTM(LSTM_UNITS, return_sequences=True))\n",
    "        \n",
    "        self.decoder_layers = [TransformerDecoderLayer(EMBEDDING_DIM, NUM_HEADS, FF_DIM) for _ in range(3)]\n",
    "        self.final_attention = layers.MultiHeadAttention(NUM_HEADS, EMBEDDING_DIM)\n",
    "        self.final_dense = layers.Dense(EMBEDDING_DIM, activation='relu')\n",
    "        self.output_layer = layers.Dense(vocab_size, activation='softmax')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        image, question = inputs\n",
    "        \n",
    "        # Image encoding\n",
    "        image_features = self.image_model(image)\n",
    "        image_features = layers.GlobalAveragePooling2D()(image_features)\n",
    "        image_features = self.image_dense(image_features)\n",
    "        image_features = tf.expand_dims(image_features, 1)\n",
    "        \n",
    "        # Question encoding\n",
    "        embedded_question = self.embedding(question)\n",
    "        question_features = self.lstm(embedded_question)\n",
    "        \n",
    "        # Combine image and question features\n",
    "        decoder_input = tf.concat([image_features, question_features], axis=1)\n",
    "        \n",
    "        # Multiple Transformer decoder layers\n",
    "        for decoder_layer in self.decoder_layers:\n",
    "            decoder_input = decoder_layer(decoder_input, decoder_input, training=True)\n",
    "        \n",
    "        # Final attention and processing\n",
    "        attention_output = self.final_attention(decoder_input, decoder_input, decoder_input)\n",
    "        output = self.final_dense(attention_output)\n",
    "        output = tf.reduce_mean(output, axis=1)  # Global average pooling\n",
    "        output = self.output_layer(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "data_df_k1000 = pd.read_csv(\"./0Data/mscoco_train2014_preprocessed_k1000.csv\")\n",
    "X = data_df_k1000[['image_id', 'question_preprocessed', 'answer_preprocessed']]\n",
    "y = data_df_k1000['answer_preprocessed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize text\n",
    "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(X['question_preprocessed'].tolist() + y.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data generators\n",
    "img_dir = \"./0Data/MSCOCO/\"  # MSCOCOの画像が保存されているディレクトリパスを指定してください\n",
    "train_generator = DataGenerator(X, tokenizer, img_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(314499, 3) (314499,)\n",
      "(38828, 3) (38828,)\n",
      "(34945, 3) (34945,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# perform train validation & test split on the dataset\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.10, stratify=y, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.10, stratify=y_train, random_state=42)\n",
    "\n",
    "\n",
    "#X_train,y_train = pickle.load(open('./0Data/train_1129.pkl', 'rb'))\n",
    "#X_val,y_val = pickle.load(open('./0Data/val_1129.pkl', 'rb'))\n",
    "#X_test,y_test = pickle.load(open('./0Data/test_1129.pkl', 'rb'))\n",
    "\n",
    "\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_val.shape, y_val.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and compile model\n",
    "model = VQAModel(MAX_VOCAB_SIZE, MAX_SEQUENCE_LENGTH)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-16 17:29:12.591345: I external/local_xla/xla/service/service.cc:168] XLA service 0x5614e87ec460 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-10-16 17:29:12.591434: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA A100-SXM4-40GB, Compute Capability 8.0\n",
      "2024-10-16 17:29:12.591450: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (1): NVIDIA A100-SXM4-40GB, Compute Capability 8.0\n",
      "2024-10-16 17:29:12.591462: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (2): NVIDIA A100-SXM4-40GB, Compute Capability 8.0\n",
      "2024-10-16 17:29:12.591473: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (3): NVIDIA A100-SXM4-40GB, Compute Capability 8.0\n",
      "2024-10-16 17:29:12.674030: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1729067352.993628  111349 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6067/6067 [==============================] - 2217s 364ms/step - loss: 0.0506 - accuracy: 0.9997\n",
      "Epoch 2/10\n",
      "6067/6067 [==============================] - 2036s 336ms/step - loss: 2.5140e-07 - accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "6067/6067 [==============================] - 2011s 331ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "6067/6067 [==============================] - 2028s 334ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "6067/6067 [==============================] - 2055s 339ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "6067/6067 [==============================] - 2035s 335ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "6067/6067 [==============================] - 2057s 339ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "6067/6067 [==============================] - 2048s 338ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "6067/6067 [==============================] - 2033s 335ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "6067/6067 [==============================] - 2045s 337ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "history = model.fit(train_generator, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./0Data/model/gpu/model_t51016/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./0Data/model/gpu/model_t51016/assets\n"
     ]
    }
   ],
   "source": [
    "model.save('./0Data/model/gpu/model_t51016', save_format=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('./0Data/model/gpu/model_t51016_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 113ms/step\n",
      "predictions shape: (1, 10000)\n",
      "predicted_seq shape: (1,)\n",
      "Question: is the sky blue\n",
      "Generated Answer: gym\n"
     ]
    }
   ],
   "source": [
    "def generate_answer(image_id, question):\n",
    "    image_path = os.path.join(img_dir, f\"{image_id}\")\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
    "    img = np.array(img) / 255.0\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    \n",
    "    question_seq = tokenizer.texts_to_sequences([question])\n",
    "    question_padded = pad_sequences(question_seq, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    \n",
    "    predictions = model.predict([img, question_padded])\n",
    "    predicted_seq = np.argmax(predictions, axis=-1)  # axis=-1 を追加\n",
    "    \n",
    "    # デバッグ用の出力\n",
    "    print(\"predictions shape:\", predictions.shape)\n",
    "    print(\"predicted_seq shape:\", predicted_seq.shape)\n",
    "    \n",
    "    # predicted_seq が2次元の場合、最初の（そして唯一の）シーケンスを取得\n",
    "    if len(predicted_seq.shape) == 2:\n",
    "        predicted_seq = predicted_seq[0]\n",
    "    \n",
    "    # predicted_seq をリストに変換\n",
    "    predicted_seq_list = predicted_seq.tolist()\n",
    "    \n",
    "    predicted_answer = tokenizer.sequences_to_texts([predicted_seq_list])[0]\n",
    "    \n",
    "    return predicted_answer\n",
    "\n",
    "# 使用例\n",
    "sample_image_id = X['image_id'].iloc[2]\n",
    "sample_question = X['question_preprocessed'].iloc[2]\n",
    "generated_answer = generate_answer(sample_image_id, sample_question)\n",
    "print(f\"Question: {sample_question}\")\n",
    "print(f\"Generated Answer: {generated_answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 24ms/step\n",
      "predictions shape: (1, 10000)\n",
      "predicted_seq shape: (1,)\n",
      "predicted_seq: [3047]\n",
      "Top 5 predictions:\n",
      "  gym: 0.0002\n",
      "  grumpy: 0.0002\n",
      "  televisions: 0.0002\n",
      "  repeat: 0.0002\n",
      "  snows: 0.0002\n",
      "Question: is this man a professional baseball player\n",
      "Generated Answer: gym\n"
     ]
    }
   ],
   "source": [
    "def generate_answer(image_id, question):\n",
    "    image_path = os.path.join(img_dir, f\"{image_id}\")\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
    "    img = np.array(img) / 255.0\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    \n",
    "    question_seq = tokenizer.texts_to_sequences([question])\n",
    "    question_padded = pad_sequences(question_seq, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    \n",
    "    predictions = model.predict([img, question_padded])\n",
    "    predicted_seq = np.argmax(predictions, axis=-1)\n",
    "    \n",
    "    print(\"predictions shape:\", predictions.shape)\n",
    "    print(\"predicted_seq shape:\", predicted_seq.shape)\n",
    "    print(\"predicted_seq:\", predicted_seq)\n",
    "    \n",
    "    # Top 5の予測結果を表示\n",
    "    top_5_indices = np.argsort(predictions[0])[-5:][::-1]\n",
    "    print(\"Top 5 predictions:\")\n",
    "    for idx in top_5_indices:\n",
    "        word = tokenizer.index_word.get(idx, \"<UNK>\")\n",
    "        prob = predictions[0][idx]\n",
    "        print(f\"  {word}: {prob:.4f}\")\n",
    "    \n",
    "    predicted_seq_list = predicted_seq.tolist()\n",
    "    predicted_answer = tokenizer.sequences_to_texts([predicted_seq_list])[0]\n",
    "    \n",
    "    return predicted_answer\n",
    "\n",
    "# 使用例\n",
    "sample_image_id = X['image_id'].iloc[0]\n",
    "sample_question = X['question_preprocessed'].iloc[0]\n",
    "generated_answer = generate_answer(sample_image_id, sample_question)\n",
    "print(f\"Question: {sample_question}\")\n",
    "print(f\"Generated Answer: {generated_answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(image_id, question):\n",
    "    image_path = os.path.join(img_dir, f\"{image_id}\")\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
    "    img = np.array(img) / 255.0\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    \n",
    "    question_seq = tokenizer.texts_to_sequences([question])\n",
    "    question_padded = pad_sequences(question_seq, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    \n",
    "    predictions = model.predict([img, question_padded])\n",
    "    predicted_token = np.argmax(predictions[0])\n",
    "    \n",
    "    print(\"predictions shape:\", predictions.shape)\n",
    "    print(\"predicted_token:\", predicted_token)\n",
    "    \n",
    "    # Top 5の予測結果を表示\n",
    "    top_5_indices = np.argsort(predictions[0])[-5:][::-1]\n",
    "    print(\"Top 5 predictions:\")\n",
    "    for idx in top_5_indices:\n",
    "        word = tokenizer.index_word.get(idx, \"<UNK>\")\n",
    "        prob = predictions[0][idx]\n",
    "        print(f\"  {word}: {prob:.4f}\")\n",
    "    \n",
    "    predicted_answer = tokenizer.index_word.get(predicted_token, \"<UNK>\")\n",
    "    \n",
    "    return predicted_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/data/t32303m/anaconda3/envs/KIBU3/lib/python3.9/site-packages/keras/src/engine/training.py\", line 2440, in predict_function  *\n        return step_function(self, iterator)\n    File \"/data/t32303m/anaconda3/envs/KIBU3/lib/python3.9/site-packages/keras/src/engine/training.py\", line 2425, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/data/t32303m/anaconda3/envs/KIBU3/lib/python3.9/site-packages/keras/src/engine/training.py\", line 2413, in run_step  **\n        outputs = model.predict_step(data)\n    File \"/data/t32303m/anaconda3/envs/KIBU3/lib/python3.9/site-packages/keras/src/engine/training.py\", line 2381, in predict_step\n        return self(x, training=False)\n    File \"/data/t32303m/anaconda3/envs/KIBU3/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/tmp/__autograph_generated_filegpfkik1x.py\", line 17, in tf__call\n        decoder_input = ag__.converted_call(ag__.ld(tf).concat, ([ag__.ld(image_features), ag__.ld(question_features)],), dict(axis=1), fscope)\n\n    ValueError: Exception encountered when calling layer 'vqa_model_4' (type VQAModel).\n    \n    in user code:\n    \n        File \"/tmp/ipykernel_110627/3008830667.py\", line 31, in call  *\n            decoder_input = tf.concat([image_features, question_features], axis=1)\n    \n        ValueError: Dimension 0 in both shapes must be equal, but are 256 and 512. Shapes are [256] and [512]. for '{{node vqa_model_4/concat}} = ConcatV2[N=2, T=DT_FLOAT, Tidx=DT_INT32](vqa_model_4/ExpandDims, vqa_model_4/bidirectional/concat, vqa_model_4/concat/axis)' with input shapes: [?,1,256], [?,50,512], [] and with computed input tensors: input[2] = <1>.\n    \n    \n    Call arguments received by layer 'vqa_model_4' (type VQAModel):\n      • inputs=('tf.Tensor(shape=(None, 224, 224, 3), dtype=float32)', 'tf.Tensor(shape=(None, 50), dtype=int32)')\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [41]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m sample_image_id \u001b[38;5;241m=\u001b[39m X[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      3\u001b[0m sample_question \u001b[38;5;241m=\u001b[39m X[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion_preprocessed\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m----> 4\u001b[0m generated_answer \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_answer\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_image_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_question\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuestion: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msample_question\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated Answer: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgenerated_answer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Input \u001b[0;32mIn [40]\u001b[0m, in \u001b[0;36mgenerate_answer\u001b[0;34m(image_id, question)\u001b[0m\n\u001b[1;32m      8\u001b[0m question_seq \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mtexts_to_sequences([question])\n\u001b[1;32m      9\u001b[0m question_padded \u001b[38;5;241m=\u001b[39m pad_sequences(question_seq, maxlen\u001b[38;5;241m=\u001b[39mMAX_SEQUENCE_LENGTH)\n\u001b[0;32m---> 11\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion_padded\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m predicted_token \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(predictions[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredictions shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, predictions\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/anaconda3/envs/KIBU3/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filekajnym28.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__predict_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filegpfkik1x.py:17\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     15\u001b[0m embedded_question \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39membedding, (ag__\u001b[38;5;241m.\u001b[39mld(question),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m question_features \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mlstm, (ag__\u001b[38;5;241m.\u001b[39mld(embedded_question),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m---> 17\u001b[0m decoder_input \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mconcat, ([ag__\u001b[38;5;241m.\u001b[39mld(image_features), ag__\u001b[38;5;241m.\u001b[39mld(question_features)],), \u001b[38;5;28mdict\u001b[39m(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), fscope)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_state\u001b[39m():\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (decoder_input,)\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/data/t32303m/anaconda3/envs/KIBU3/lib/python3.9/site-packages/keras/src/engine/training.py\", line 2440, in predict_function  *\n        return step_function(self, iterator)\n    File \"/data/t32303m/anaconda3/envs/KIBU3/lib/python3.9/site-packages/keras/src/engine/training.py\", line 2425, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/data/t32303m/anaconda3/envs/KIBU3/lib/python3.9/site-packages/keras/src/engine/training.py\", line 2413, in run_step  **\n        outputs = model.predict_step(data)\n    File \"/data/t32303m/anaconda3/envs/KIBU3/lib/python3.9/site-packages/keras/src/engine/training.py\", line 2381, in predict_step\n        return self(x, training=False)\n    File \"/data/t32303m/anaconda3/envs/KIBU3/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/tmp/__autograph_generated_filegpfkik1x.py\", line 17, in tf__call\n        decoder_input = ag__.converted_call(ag__.ld(tf).concat, ([ag__.ld(image_features), ag__.ld(question_features)],), dict(axis=1), fscope)\n\n    ValueError: Exception encountered when calling layer 'vqa_model_4' (type VQAModel).\n    \n    in user code:\n    \n        File \"/tmp/ipykernel_110627/3008830667.py\", line 31, in call  *\n            decoder_input = tf.concat([image_features, question_features], axis=1)\n    \n        ValueError: Dimension 0 in both shapes must be equal, but are 256 and 512. Shapes are [256] and [512]. for '{{node vqa_model_4/concat}} = ConcatV2[N=2, T=DT_FLOAT, Tidx=DT_INT32](vqa_model_4/ExpandDims, vqa_model_4/bidirectional/concat, vqa_model_4/concat/axis)' with input shapes: [?,1,256], [?,50,512], [] and with computed input tensors: input[2] = <1>.\n    \n    \n    Call arguments received by layer 'vqa_model_4' (type VQAModel):\n      • inputs=('tf.Tensor(shape=(None, 224, 224, 3), dtype=float32)', 'tf.Tensor(shape=(None, 50), dtype=int32)')\n"
     ]
    }
   ],
   "source": [
    "# 使用例\n",
    "sample_image_id = X['image_id'].iloc[0]\n",
    "sample_question = X['question_preprocessed'].iloc[0]\n",
    "generated_answer = generate_answer(sample_image_id, sample_question)\n",
    "print(f\"Question: {sample_question}\")\n",
    "print(f\"Generated Answer: {generated_answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer word_index: [('<OOV>', 1), ('the', 2), ('is', 3), ('what', 4), ('are', 5), ('yes', 6), ('no', 7), ('this', 8), ('in', 9), ('a', 10)]\n",
      "Tokenizer index_word: [(1, '<OOV>'), (2, 'the'), (3, 'is'), (4, 'what'), (5, 'are'), (6, 'yes'), (7, 'no'), (8, 'this'), (9, 'in'), (10, 'a')]\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokenizer word_index:\", list(tokenizer.word_index.items())[:10])\n",
    "print(\"Tokenizer index_word:\", list(tokenizer.index_word.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'dense_16', 'trainable': True, 'dtype': 'float32', 'units': 10000, 'activation': 'softmax', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n"
     ]
    }
   ],
   "source": [
    "print(model.layers[-1].get_config())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "KIBU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
